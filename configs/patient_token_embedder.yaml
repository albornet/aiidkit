# Argument for HuggingFace's trainer
TRAINING_ARGUMENTS:

  # Training parameters
  logging_steps: 10
  warmup_steps: 1000  # warmup_ratio: 0.1?
  num_train_epochs: 250  # maximum number of epochs (technically never reached)
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1  # try to amount to 32 (or more?)
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string
  weight_decay: 0.1
  bf16: true

  # Evaluation parameters (for trainer)
  per_device_eval_batch_size: 128
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  load_best_model_at_end: true
  # include_for_metrics: [loss]
  report_to: wandb

  # Paths and names
  wandb_project: pretraining_base_bact  # only used if report_to == wandb
  run_name: pretraining_base_bact
  output_dir: results/pretraining_base_bact
  logging_dir: results/pretraining_base_bact/logs

# Argument for my own evaluator class
EVALUATION_ARGUMENTS:

  metric_for_best_model: sup_roc_auc  # sup_f1_macro, sup_pr_auc, sup_roc_auc
  greater_is_better: true             # whether the metric for best model should be maximized or minimized
  early_stopping_patience: 20         # how many evaluations without improvement in metric for best model before stopping
  do_clustering_analysis: false       # within evaluation loop, whether reduce and cluster embeddings
  optuna_trials_for_clustering: 25    # within evaluation loop, None if you don't want to use optuna

# Model arguments
MODEL_ARGUMENTS:

  # Model parameters
  original_model_id: answerdotai/ModernBERT-base  # note: "-large" tended to overfit
  original_model_task: masked                     # masked, causal (causal not fully implemented yet)
  original_model_params:                          # any parameter to modify from the original model
    output_hidden_states: true  # keep this in any case
    torch_dtype: bfloat16       # keep this in any case
    # num_hidden_layers: 8      #   4 //    8 //   12
    # num_attention_heads: 8    #   6 //    8 //   12
    # hidden_size: 256          # 120 //  256 //  512
    # intermediate_size: 1024   # 480 // 1024 // 2048

  # Input layer parameters
  use_positional_encoding_for_input_layer: true     # true, false
  use_pretrained_embeddings_for_input_layer: false  # true, false
  pretrained_model_name_for_input_layer: NeuML/pubmedbert-base-embeddings  # sentence embedding model fine-tuned on clinical data

  # Output layer parameters task
  supervised_task_key: infection_label_binary_bacterial  # must correspond to below!
  supervised_task_type: binary       # binary (2), categorical (4), not sure if one-hot can work seamlessly
  use_supervised_task: true          # true, false
  use_uncertainty_weighting: false   # true, false -> to combine mlm and supervised task losses
  supervised_task_schedule:          # only used if use_uncertainty weighting is false!
    start_steps: 2000  # 10000  # n_steps before classification weight starts moving towards end_value
    end_steps: 4000  # 12000    # n_steps after which classification weight reaches end_value
    start_value: 0.001          # classification weight at the beginning of training
    end_value: 1.0              # classification weight at the end of training
    strategy: "cosine"          # annealing strategy, "linear", "cosine"
