# Dataset building
DEBUG: false
BUILD_PATIENT_CSV_FILES: true
BUILD_HUGGINGFACE_DATASET: false
RESULT_DIR: results
RAW_DATA_DIR: raw_data
RAW_DATA_SUBSET: eicu
PROCESSED_DATA_DIR: processed_data

# Training arguments
DEFAULT_NUM_VALUE_BINS: 10
TUNE_NUM_VALUE_BINS: false
NUM_ADDED_TOKENS: 4
NUM_TRAIN_EPOCHS: 1000
DEFAULT_TRAINING_ARGUMENTS:
  output_dir: results
  logging_dir: results/logs
  eval_strategy: steps
  save_strategy: steps
  save_total_limit: 1
  logging_steps: 10
  eval_steps: 100
  save_steps: 100
  warmup_steps: 100
  learning_rate: 0.00005
  bf16: true
  weight_decay: 0.01
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  num_train_epochs: 1000  # -> 1 if DEBUG == true
  load_best_model_at_end: true
  metric_for_best_model: monitored_metric
  greater_is_better: true

# Training utilities
USE_OPTUNA: true
NUM_OPTUNA_TRIALS: 100
TRAINER_CALLBACKS:
  - EarlyStoppingCallback:
      early_stopping_patience: 4
      early_stopping_threshold: 0.01

# TODO: HAVE ONE CONFIG PER MODEL!!! AND POTENTIALLY ONE GENERAL CONFIG
# Model
LM_TYPE: masked
LM_PRETRAINING: healthcare
POSSIBLE_MODELS:
  masked:
    general:
      - bert-base-uncased
      - roberta-base
      - distilbert-base-uncased
    healthcare:
      - microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext
      - emilyalsentzer/Bio_ClinicalBERT
      - dmis-lab/biobert-base-cased-v1.1
  causal:
    general:
      - gpt2
      - distilgpt2
      - EleutherAI/gpt-neo-2.7B
    healthcare:
      - microsoft/BioGPT
      - lucadiliello/clinical-gpt
      - stanford-crfm/pubmedgpt
