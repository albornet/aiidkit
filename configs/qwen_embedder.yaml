# Model configuration
model:
  model_name: Qwen/Qwen3-Embedding-0.6B
  # model_name: Qwen/Qwen3-Embedding-4B
  model_kwargs:
    trust_remote_code: true
    attn_implementation: flash_attention_2
    torch_dtype: bfloat16
    device_map: auto
  # For debug
  max_seq_length: 512
  truncation_side: left

# Data processing parameters
data:
  max_days_after_first_transplant: 60
  use_markdown: true

# Training configuration
train:

  # General settings
  output_dir: results/qwen3_embedding
  true_label_key: infection_label_binary
  task_description: "Predict the patient's infection risk in the next ##HORIZON## days"  # not used for now
  true_label_classes:  # not used for now
    - "Healthy in the next ##HORIZON## days"
    - "Infected in the next ##HORIZON## days"

  # LoRA: standard PEFT LoRA settings
  use_lora: true
  lora:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.05
      task_type: FEATURE_EXTRACTION  # required by sentence-transformer models (?)
      bias: none
      target_modules: [q_proj, k_proj, v_proj, o_proj]

  # QLoRA: enables 4-bit quantization for memory savings
  quantization:
    use_4bit: true
    bnb_config:
      load_in_4bit: true
      bnb_4bit_quant_type: nf4
      bnb_4bit_compute_dtype: bfloat16
      bnb_4bit_use_double_quant: true

  # Arguments for the SentenceTransformerTrainer
  training_arguments:
    num_train_epochs: 10
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 4
    per_device_eval_batch_size: 1
    learning_rate: 1.0e-5
    warmup_steps: 500
    eval_strategy: steps
    eval_steps: 250
    save_strategy: steps
    save_steps: 250
    logging_steps: 50
    save_total_limit: 1
    bf16: true
    run_name: qwen3_embeddings
