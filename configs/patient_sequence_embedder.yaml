# Arguments for the sentence-transformer's trainer
TRAINING_ARGUMENTS:
  output_dir: results/finetuning
  logging_dir: results/finetuning/logs
  logging_steps: 10
  num_train_epochs: 10
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1  # try to amount to 16
  warmup_steps: 1000  # warmup_ratio: 0.1?
  learning_rate: 1.0e-5  # careful! 1e-5 will be read as a string
  weight_decay: 0.01
  bf16: true
  
  # Evaluation settings
  per_device_eval_batch_size: 16
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1

# Parameters for the custom clustering evaluation
EVALUATION_ARGUMENTS:
  optuna_trials: 25
  infection_days_low: 0
  infection_days_high: 300
