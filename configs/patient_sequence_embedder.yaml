# Arguments for the sentence-transformer's trainer
TRAINING_ARGUMENTS:
  output_dir: results/finetuning
  logging_dir: results/finetuning/logs
  logging_steps: 10
  num_train_epochs: 100
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1  # try to amount to 16
  warmup_steps: 1000  # warmup_ratio: 0.1?
  learning_rate: 1.0e-5  # careful! 1e-5 will be read as a string
  weight_decay: 0.01
  bf16: true
  
  # Evaluation settings
  per_device_eval_batch_size: 16
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 1

# Parameters for the custom clustering evaluation
EVALUATION_ARGUMENTS:
  optuna_trials: 10
  early_stopping_patience: 10  # 5
  eval_label_key: infection_label_binary

# External training arguments
SUPERVISED_LABEL_NAMES:
  - infection_label_binary
  # - infection_label_categorical
  # - sequence_id